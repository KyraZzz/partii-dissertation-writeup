% allocate 9 pages
\chapter{Evaluation} 
\label{sec:evaluation}
This chapter analyses two research questions proposed in \Cref{section:motivation} through quantitative and qualitative methods. The first question is addressed in \Cref{sec:eval-prompt}, where prompting model performance in a few-shot learning scenario is compared. The second question is explored in \Cref{sec:eval-backdoor}, demonstrating the vulnerability of each prompting model to different backdoor attack settings. Additionally, \Cref{sec:eval-success} evaluates how the success criteria for both the core project and its extensions are met.

\section{Prompting Model Performance Analysis} \label{sec:eval-prompt}
\subsection{Prompt \& Verbaliser Designs}
\subsubsection{Designs In Manual Prompting} \label{sec:eval-manul-prompt}
For each of the six datasets, four to six manual prompt-and-verbaliser pairs are chosen from either the Public Pool of Prompts \cite{Bach22OPP} or previous literature on prompting \cite{Gao20PM, Lei22}. The performance is compared under the same $K = 16$ few-shot scenario. Detailed results can be found in \Cref{tab:manual_select}. The mean and standard deviation of the classification performance score (Accuracy or F1) are computed across five independent runs with different random seeds (e.g., \texttt{seed $\in$ \{13, 21, 42, 87, 100\}}) using the same experimental setting. 

\input{tables/manual_select}
The prompt design varies based on dataset characteristics. For textual entailment datasets (e.g., \textit{QNLI}, \textit{MNLI-MATCHED}, and \textit{MNLI-MISMATCHED}), the $<$\textit{mask}$>$ token is placed between the input text pair. For sentiment analysis datasets (e.g., \textit{SST2}, \textit{ENRON-SPAM}, and \textit{TWEETS-HATE-OFFENSIVE}), the $<$\textit{mask}$>$ token is placed on either side of the single input text. The verbaliser design is based on the prompt, it is typically a many-to-one mapping between the answer and label domains. However, \Cref{sec:diff-prompt} specifies that differential prompting requires a one-to-one mapping between trainable embeddings and labels. Thus, all prompting models choose a one-to-one mapping verbaliser to ensure a fair comparison of prompting models, using words that typically express opposing sentiments in the answer domain.

The best-performing pair with the highest mean score is picked for further experiments with different $K$ values, as listed in \Cref{tab:manual-best-prompts}. This is because initially, we have $K = \{16, 100, 1000\}$ and since the project considers few-shot learning scenarios, $K = 16$ is the focus. 
\input{tables/manual-best-prompts}

\vspace{-1.0em}
\subsubsection{Designs In Auto Prompting} \label{sec:eval-auto}
An automated discrete prompt utilises trigger tokens $<$T$>$ and a $<$\textit{mask}$>$ token. Following the same settings used in AutoPrompt \cite{shin2020autoprompt}, we inserted ten trigger tokens between the input text and the $<$\textit{mask}$>$ token, as shown in \Cref{tab:auto_prompt_subset}. Before model training, a label search procedure is applied to generate a suitable verbaliser for the prompt automatically. The verbaliser answer-label mapping is constructed for each  
$K$-shot scenario ($K \in \{16, 100, 1000\}$) using the train and validation dataset, each with $K$ samples per class.

\input{tables/auto_prompt_subset}

\vspace{-0.6em}
\Cref{tab:auto_prompt_subset} displays the automated discrete prompt and verbaliser designs for four datasets across different $K$-shot scenarios; more results can be found in \Cref{sec:appendix-auto-prompt-designs}. Although the verbaliser is generated in a way to select the most contextually relevant words for the corresponding labels, the answer domains of the verbaliser may contain noise in some instances, such as when $K$ is small or input samples within the same class lack shared vocabulary. 

Small values of $K$ result in limited samples in the train and validation datasets, which can hinder the label search procedure described in \Cref{sec:auto-verb} from generalising effectively. For instance, for the \textit{TWEETS-HATE-OFFENSIVE} dataset, $K=16$ leads to the association of the answer \texttt{kicking} with hate speech (i.e., \texttt{kicking} $\mapsto$ 0), while larger values of $K$ yield more representative and general terms such as \texttt{racist} and \texttt{homophobia}.

When input samples within the same class lack common vocabulary, such as in the \textit{ENRON-SPAM} dataset with $K = 16$, spam emails often contain words highly related to \texttt{Discount}, whereas genuine emails do not share any suitable common words. As a result, the answer domain for the class representing genuine emails contains noisy terms such as \texttt{debian} and \texttt{subcommittee}. The same challenge is present in the textual entailment datasets such as \textit{QNLI} and \textit{MNLI-MATCHED}, where input samples are inherently dissimilar, making it difficult to interpret the generated verbaliser.

\vspace{-0.3em}
\subsubsection{Designs In Differential Prompting}
Following the DART framework \cite{zhang2021differentiable}, differential prompting employs prompt-and-verbaliser pairs from the manually designed pairs in \Cref{sec:eval-manul-prompt}, then treats the discrete tokens in the template and the verbaliser answer domain as a collection of trainable embeddings.

\subsection{Quantitative Performance Analysis}
\Cref{tab:prompt_perform} compares the classification performance of fine-tuning (Baseline) with different prompting models (Manual, Auto, and Diff). Each setting underwent five independent runs, and the mean and standard deviation of Accuracy or F1 score were computed.

Among 18 cases involving 6 datasets and 3 different $K$-shot learning scenarios ($K \in \{16, 100, 100\}$), Manual outperforms all other models in 10 cases and ranks second in 6 cases. Diff emerges as the top-performing model in 5 cases and secures the second-best position in 6 cases. Diff and Manual have comparable performance scores in most cases, with four exceptions where they significantly differ. Auto performs relatively poorly for small values of $K$ and only shows a clear advantage in \textit{TWEETS-HATE-OFFENSIVE} when $K = 100$. Performance converges with increasing $K$, for large values of $K$, models show comparable performance.  

\input{tables/prompt_perform}
In conclusion, automated prompting models do not consistently outperform manual prompting models across various tasks. Furthermore, the efficacy of the automated discrete prompting model relies heavily on the available data, and in specific scenarios, it may underperform compared to fine-tuning.

\subsection{An Extended K-value Range (Extension)} \label{sec:eval_more_k}
To further compare the performance of different prompting models and the fine-tuning method under a few-shot learning scenario, we expanded the $K$ values from $\{16, 100, 1000\}$ to $\{8, 16, 32, 64, 100, 1000\}$, covering more small $K$ values.

\vspace{-0.3em}
\begin{figure}[!ht]
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/SST2_prompting_performance.pdf}
  \caption{SST2}
  \label{fig:sst}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/QNLI_prompting_performance.pdf}
  \caption{QNLI}
  \label{fig:qnli}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/MNLI-MATCHED_prompting_performance.pdf}
  \caption{MNLI-MATCHED}
  \label{fig:matched}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/MNLI-MISMATCHED_prompting_performance.pdf}
  \caption{MNLI-MISMATCHED}
  \label{fig:mismatched}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/ENRON-SPAM_prompting_performance.pdf}
  \caption{ENRON-SPAM}
  \label{fig:enron}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/TWEETS-H-O_prompting_performance.pdf}
  \caption{TWEETS-H-O}
  \label{fig:tweets}
\end{subfigure}
\caption{The performance of prompting models on six datasets for a wider range of $K$ values. The solid line plots the mean Accuracy or F1 score across five independent runs, and is bounded by one standard deviation on both sides.}
\label{fig:more_k}
\end{figure}

\vspace{-0.3em}
\Cref{fig:more_k} demonstrates that when $K$ increases, model performance converges to a similar level. However, Manual outperforms others in few-shot scenarios, except for the \textit{SST2} dataset. Considering experimental randomness, Diff and Manual have comparable performance, except for the \textit{QNLI} dataset, where Manual significantly outperforms Diff. Regarding model stability, prompting models are more stable than the fine-tuning approach, as they have a smaller standard deviation. For \textit{ENRON-SPAM} and \textit{TWEETS-HATE-OFFENSIVE} datasets, the performance of Baseline, Manual and Diff are comparable. 

The figures also reveal that Auto consistently performs poorly in most few-shot scenarios. As explained in \Cref{sec:eval-auto}, the label search procedure in Auto generates noisy answer domains when $K$ is small or when the input samples from the same class lack common vocabulary. A noisy answer domain may contain answers that are not distantly apart semantically; hence the prompting performance may be impacted heavily.

\vspace{-0.5em}
\section{Backdoor Attack Performance Analysis} \label{sec:eval-backdoor}
\vspace{-0.4em}
\subsection{Quantitative Backdoor Attack Analysis}
\vspace{-0.2em}
Backdoor attacks are launched onto the prompting models using six triggers \texttt{\{"cf", "mn", "bb", "qt", "pt", "mt"\}}. For each prompting model (Manual, Auto, Diff), we measured the change in classification performance (ACC $\Delta$ or F1 $\Delta$) between backdoored and non-backdoored versions. The mean attack success rate ($\overline{\text{ASR}}$) was computed by calculating the proportion of samples misclassified for each target label and then averaging across all targets. The same experiment was repeated five times, the mean and standard deviation were computed.

\input{tables/backdoor_16}
According to \Cref{tab:backdoor-16}, all three models demonstrate comparable Accuracy or F1 scores between the backdoored and non-backdoored versions under $K = 16$, as indicated by the small ACC $\Delta$ values. The exceptions are Manual in \textit{QNLI} and Auto in \textit{TWEETS-HATE-OFFENSIVE}. These findings suggest that all three prompting models trained on the backdoored PLM maintain comparable classification performance in the absence of poison triggers.

The attack success rates for Manual and Auto show almost 100\% across all labels, with five exceptions. For instance, in \textit{MNLI-MATCHED}, Manual achieves a low $\overline{\text{ASR}}$ because ASR for label 0 is nearly 0\%, while ASRs for labels 1 and 2 are 100\%. This result suggests that none of the target embeddings of the poison triggers is associated with label 0, indicating insufficient coverage of the set of six poison triggers.

Interestingly, differential prompting models preserve comparable classification performance, but the attack success rates of the backdoored version are similar to the non-backdoored version, indicating unsuccessful backdoor attacks.

To investigate further, we compare performance for multiple $K$-shot learning scenarios ($K = \{16, 100, 1000\}$). The results in \Cref{fig:score_n_attack} and \Cref{sec:appendix-backdoor-perform} demonstrate that backdoor attacks on all prompting models can maintain comparable classification performance, and changes in $K$ have minimal impact on classification performance differences. Most backdoored versions show less than $5\%$ differences (ACC $\Delta$ or F1 $\Delta$) compared to their non-backdoored counterparts. However, some exceptions were observed, such as Manual in \textit{TWEETS-HATE-OFFENSIVE} at $K = 100$, which exhibited notable increases in F1 $\Delta$, indicating superior performance than the non-backdoored version. Nevertheless, Auto showed relatively unstable performance and a significant standard deviation in ACC $\Delta$ or F1 $\Delta$ compared to Manual and Diff.

Regarding attack success rates, Diff is more robust than Manual and Auto in all six datasets, as $\overline{\text{ASR}}$ in Diff is consistently lower than random guess (i.e., $50\%$). While Manual and Auto achieve precisely or nearly $100\%$ ASR for at least one of the target labels in all cases, there are instances where the backdoor triggers may not cover all target labels, resulting in a lower $\overline{\text{ASR}}$, such as Auto in QNLI when $K = 16$.

\begin{figure}[!ht]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/SST2_score_n_attack.pdf}
  \caption{SST2}
  \label{fig:sst}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/TWEETS-HATE-OFFENSIVE_score_n_attack.pdf}
  \caption{TWEETS-H-O}
  \label{fig:tweets}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/QNLI_score_n_attack.pdf}
  \caption{QNLI}
  \label{fig:qnli}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/MNLI-MISMATCHED_score_n_attack.pdf}
  \caption{MNLI-MISMATCHED}
  \label{fig:mismatched}
\end{subfigure}
\caption{The backdoor attack performance of three prompting models was evaluated for $K = \{16,100,1000\}$ for four datasets. Mean ACC $\Delta$ or F1 $\Delta$ measures the average difference in classification performance between the backdoored and non-backdoored versions. The bar plots and the line plots illustrate $\overline{\text{ASR}}$ across all target labels.}
\label{fig:score_n_attack}
\end{figure}

To conclude, while all prompting models demonstrate comparable classification performance, the differential prompting model displays superior robustness compared to manual and auto prompting models. This is evidenced by consistently lower mean ASR values than random guesses. In addition, although manual and auto prompting models achieve mean ASR values of 100\% in most cases, there are instances where none of the target embeddings of poison triggers is associated with a target label, resulting in an ASR close to 0\%.

\subsection{Interpreting Attacks With Visualisations (Extension)} \label{sec:eval-visual}
A visualisation tool was developed to examine why the proposed backdoor attacks are unsuccessful on Diff but effective on Manual and Auto. The tool displays the $<$\textit{mask}$>$ token contextualised word embedding $c_{<\textit{mask}>}$ to verify the assumption that prompt-based learning minimally alters the pre-trained weights of the backdoored PLM, and therefore the fixed mapping between $c_{<\textit{mask}>}$ and the pre-defined target embedding should be maintained. 

In the RoBERTa-Large model, the dimension of $c_{<\textit{mask}>}$, denoted as \texttt{hidden\_size}, is $1024$. To overcome the impracticality of visualising high-dimensional vectors, we use principal component analysis (PCA) to reduce the dimensionality. This technique transforms the embeddings into a new coordinate system, preserving the most variance in the data. The resulting vector of principal component scores $t = [t_1, ..., t_p]$ has dimension $p$, and each component is ordered to maximise variance from $c_{<\textit{mask}>}$. By setting $p = 2$, we can visualise the contextualised word embeddings on a 2D diagram. 

\vspace{-0.8em}
\input{tables/visualise_16}

The 2D plots of the $<$\textit{mask}$>$ token contextualised embeddings in the \textit{SST2} dataset with $K = 16$ are displayed in \Cref{fig:visualise_16}. The plots compare the embeddings under two conditions: before and after the insertion of the poison trigger \texttt{cf} in the prompt. 

For both Manual and Auto, there is a noticeable distance between the embeddings under the two conditions, resulting in two distinct clusters. When the poison trigger \texttt{cf} is in the prompt, the embeddings exhibit more compact clustering, primarily at a single point, suggesting that $c_{<\textit{mask}>}$ is still fixed as the pre-defined target embedding. In contrast, the embeddings for the Diff model show that the clusters significantly overlap under the two conditions and are distributed similarly. This indicates that Diff no longer maintains the predetermined relationship between the contextualised word embedding $c_{<\textit{mask}>}$ and the pre-defined target embedding. Additional 2D plots for all six datasets are available in \Cref{sec:appendix-visual}, and they display similar patterns.

\Cref{fig:visualise_1000} in \Cref{sec:appendix-visual} demonstrates the $<$\textit{mask}$>$ token contextualised embeddings in the \textit{SST2} dataset with $K = 1000$. We observe that adjusting the $K$ value has a negligible impact on cluster separation and distribution. This observation is consistent with the findings in \Cref{fig:score_n_attack}, where variations in $K$ had minimal effect on attack success rates.

\subsection{Backdoor Attacks With Different Settings (Extension)} \label{sec:eval-backdoor-setting}
In \Cref{sec:eval-backdoor}, a backdoored PLM was trained on poisoned data samples from \textit{WikiText}, using six visible triggers: \texttt{{["cf", "mn", "bb", "qt", "pt", "mt"]}} inserted at the start of the prompt. Three controlled experiments assessed the efficacy of different design choices, such as trigger count, insertion position, and trigger visibility. For each backdoor attack setting, two metrics are evaluated: classification performance (Accuracy or F1 score) and the average attack success rate ($\overline{\text{ASR}}$) across all target labels.

\begin{comment}
\begin{figure*}[!ht]
%QNLI
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/QNLI_num_trigger_impacts.pdf}
  \caption{QNLI}
  \label{fig:qnli_trigger_impacts}
\end{subfigure}%
%SST2
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/SST2_insert_pos_impacts.pdf}
  \caption{SST2}
  \label{fig:sst2_insert_pos_impacts}
\end{subfigure}%
%ENRON-SPAM
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/ENRON-SPAM_trigger_type_impacts.pdf}
  \caption{ENRON-SPAM}
  \label{fig:enron_spam_poison_ratio_impacts}
\end{subfigure}
%TWEETS
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/TWEETS-HATE-OFFENSIVE_num_trigger_impacts.pdf}
  \caption{TWEETS-H-O}
  \label{fig:tweets_trigger_impacts}
\end{subfigure}%
%MNLI-MATCHED
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/MNLI-MATCHED_insert_pos_impacts.pdf}
  \caption{MNLI-MACTHED}
  \label{fig:mnli_matched_insert_pos_impacts}
\end{subfigure}%
%MNLI-MISMATCHED
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/MNLI-MISMATCHED_trigger_type_impacts.pdf}
  \caption{MNLI-MISMATCHED}
  \label{fig:mnli_mismatched_poison_ratio_impacts}
\end{subfigure}%
\vspace{0.5em}
\caption{Controlled experiments to evaluate the efficacy of various design choices, including trigger count, insertion position and trigger visibility.}
\label{fig:eval_different_backdoor}
\end{figure*}
\end{comment}
\vspace{-1.0em}
\subsubsection{Different Number of Triggers}
\vspace{-0.6em}
The datasets chosen are the \textit{QNLI} binary textual entailment dataset and the \textit{TWEETS-HATE-OFFENSIVE} multi-class sentiment analysis dataset, as shown in \Cref{fig:eval_different_backdoor_num_trigger}. The experiments involve the use of one trigger \texttt{["cf"]}, three triggers \texttt{["cf", "mn", "bb"]}, and the original setting with all six triggers.

In both datasets, the number of triggers had minimal impact on the change in classification performance (ACC or F1 score), except Auto in \textit{TWEETS-HATE-OFFENSIVE} when $K = 16$. For attack success rates, increasing the number of trigger tokens resulted in better attack coverage and higher $\overline{\text{ASR}}$, except Auto in \textit{QNLI} when $K = 16$. Moreover, the increase in trigger numbers had a more significant impact on $\overline{\text{ASR}}$ in Manual and Auto than Diff.

\vspace{-0.3em}
\begin{figure*}[!ht]
%QNLI
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/QNLI_num_trigger_impacts.pdf}
  \caption{QNLI}
  \label{fig:qnli_trigger_impacts}
\end{subfigure}%
%TWEETS
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/TWEETS-HATE-OFFENSIVE_num_trigger_impacts.pdf}
  \caption{TWEETS-H-O}
  \label{fig:tweets_trigger_impacts}
\end{subfigure}%
\vspace{0.5em}
\caption{Controlled experiments to evaluate the efficacy of trigger count.}
\label{fig:eval_different_backdoor_num_trigger}
\end{figure*}

\vspace{-1.0em}
\subsubsection{Different Trigger Insertion Positions}
\vspace{-0.6em}
This study evaluates the impact of trigger insertion positions on classification performance and attack success rates. Three positions are considered: before the first token (\textit{Start}), before the mask token (\textit{Middle}), and after the last token (\textit{End}), as illustrated in \Cref{fig:eval_different_backdoor_insert_pos}.

\begin{figure*}[!ht]
%SST2
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/SST2_insert_pos_impacts.pdf}
  \caption{SST2}
  \label{fig:sst2_insert_pos_impacts}
\end{subfigure}%
%MNLI-MATCHED
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/MNLI-MATCHED_insert_pos_impacts.pdf}
  \caption{MNLI-MACTHED}
  \label{fig:mnli_matched_insert_pos_impacts}
\end{subfigure}%
\vspace{0.5em}
\caption{Controlled experiments to evaluate the efficacy of insertion position of trigger token.}
\label{fig:eval_different_backdoor_insert_pos}
\end{figure*}

Most cases did not exhibit performance changes, except for Auto at $K \in \{16,100\}$ when transitioning from \textit{Start} to \textit{Middle} and \textit{End}, resulting in a decrease in performance. Additionally, Manual and Auto experienced a decline in $\overline{\text{ASR}}$ when transitioning from \textit{Start} to \textit{Middle} or \textit{End}. Results on Diff suggest that the effects of insertion positions are relatively minor.

\vspace{-1.0em}
\subsubsection{Invisible Backdoor Triggers} \label{sec:eval-backdoor-invisible}
\vspace{-0.6em}
If end-users inspect the input tokens of the backdoored model during training, the use of nonsense words (e.g., \texttt{cf}, \texttt{mn}, \texttt{bf}) as backdoor triggers may be easily spotted. Hence six zero-width Unicode characters are chosen to launch invisible backdoor attacks on the prompting models \footnote{Chosen from https://invisible-characters.com/\#:$\sim$:text=Invisible}: \texttt{\{U+200B, U+200C, U+200D, U+200E, U+200F, U+2062\}}.

As shown in \Cref{fig:eval_different_backdoor_invisible_backdoor}, the performance of invisible backdoor triggers is comparable to visible ones, with the exception of a slight decrease in Manual and Auto on \textit{ENRON-SPAM} when $K = 16$. Furthermore, the $\overline{\text{ASR}}$ of invisible backdoor triggers is comparable to those of visible ones, demonstrating the effectiveness of invisible backdoor triggers.

\begin{figure*}[!ht]
%ENRON-SPAM
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/ENRON-SPAM_trigger_type_impacts.pdf}
  \caption{ENRON-SPAM}
  \label{fig:enron_spam_poison_ratio_impacts}
\end{subfigure}
%MNLI-MISMATCHED
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/evaluation_media/MNLI-MISMATCHED_trigger_type_impacts.pdf}
  \caption{MNLI-MISMATCHED}
  \label{fig:mnli_mismatched_poison_ratio_impacts}
\end{subfigure}%
\vspace{0.5em}
\caption{Controlled experiments to evaluate evaluate the efficacy of trigger visibility.}
\label{fig:eval_different_backdoor_invisible_backdoor}
\end{figure*}

\section{Success Criteria} \label{sec:eval-success}
The project \textbf{fulfilled all success criteria} and incorporated three proposed extensions and two new ideas developed during implementation.
\vspace{-1em}
\subsection{Core Project}
\vspace{-0.5em}
\begin{enumerate}[topsep=0pt, itemsep=0.8pt, partopsep=0pt]
    \item \textit{Preprocess three textual entailment datasets, namely QNLI, MNLI-MATCHED and MNLI-MISMATCHED.} \\
    \Cref{sec:dataset} outlines a data pre-processing pipeline for generating K-shot sets and preparing input samples for model training via input tokenisation. Additionally, \Cref{sec:dataset-2} illustrates a hierarchical inheritance structure for dataset-related modules, facilitating the seamless addition of new datasets.
    \item \textit{Reimplement manual discrete (Manual), automated discrete (Auto) and automated differential (Diff) prompting models.} \\
    Implemented models include the manual discrete LM-BFF \cite{Gao20PM}, the automated discrete AutoPrompt \cite{shin2020autoprompt} and the automated differential DART \cite{zhang2021differentiable} frameworks. Upon comparing the implementation results to those reported in the literature, as detailed in \Cref{sec:reprod_lit_res}, we have successfully validated the accuracy of the implementation.
    \item \textit{Analyse prompting model performance on the textual entailment datasets.} \\
    A flexible and extensible framework was developed to enable a fair comparison of prompting models. As shown in \Cref{sec:eval-prompt}, the performance of the prompting models was analysed quantitatively and qualitatively on six datasets (described in \Cref{sec:prepare-six-dataset}).
    \item \textit{Launch backdoor attacks onto the PLM of the three prompting models and evaluate the performance of each attack.} \\
    The framework is augmented to incorporate the backdoor planting process \cite{Lei22} in the pre-trained language model (PLM), as outlined in \Cref{sec:backdoor-plant}. The evaluation of backdoor performance under various settings on all three prompting models is discussed in \Cref{sec:eval-backdoor}.
\end{enumerate}
\vspace{-0.5em}
\subsection{Extensions}
\vspace{-0.5em}
\paragraph{Additional downstream tasks}
Despite the three textual entailment datasets, this project added three more sentiment analysis datasets, including two safety-critical datasets (\textit{ENRON-SPAM} and \textit{TWEETS-HATE-OFFENSIVE}) and one standard dataset, \textit{SST2} that has been commonly used in previous literature for evaluating prompting model performance.
\vspace{-1.0em}
 \paragraph{A wider range of few-shot K values}
To enhance the investigation of the prompting model performance in few-shot learning, we included a new idea to add a set of small values $K = \{8, 32, 64\}$ to the original set $K = \{16, 100, 1000\}$. Given that few-shot cases usually present a high experimental variance, including more $K$ values helps to focus on major trends and achieve more equitable comparisons, as shown in \Cref{sec:eval_more_k}.
\vspace{-1.0em}
 \paragraph{Interpreting attacks with visualisations}
The findings in \Cref{sec:eval-backdoor} suggest that Diff outperforms Manual and Auto against the proposed backdoor attacks. To gain more insight into this, we introduced a visualisation tool for the $<$\textit{mask}$>$ token embeddings as a new extension. As discussed in \Cref{sec:eval-visual}, this extension plots the embeddings onto a 2D diagram, enabling a clearer understanding of the variations in the robustness of the prompting models.
\vspace{-1.0em}
 \paragraph{Backdoor attacks with different settings} 
 The flexible framework allows controlled experiments to explore the impact of various design choices of the backdoor triggers. Three sets of experiments are conducted, as outlined in \Cref{sec:eval-backdoor-setting}, to evaluate the effectiveness of trigger count, insertion position and trigger visibility.
\vspace{-1.0em}
 \paragraph{Invisible backdoors using Unicode characters}
 Instead of nonsense words (e.g., \textit{cf}, \textit{mn}, \textit{bb}), zero-width Unicode characters (e.g., \textit{U+200B}, \textit{U+200C}) are utilised. The efficacy of these invisible backdoor triggers are evaluated in \Cref{sec:eval-backdoor-invisible}.