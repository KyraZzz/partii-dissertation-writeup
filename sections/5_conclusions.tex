% allocate 1 page
\chapter{Conclusions}
\section{Achievements} 
An adaptable framework was developed to assess prompting model performance and analyse the impacts of backdoor attacks, effectively addressing the research questions presented in \Cref{section:motivation}. Three prompting models were re-implemented: the manual discrete LM-BFF \cite{Gao20PM}, the automated discrete AutoPrompt \cite{shin2020autoprompt}, and the automated differential DART \cite{zhang2021differentiable} models.

AutoPrompt did not address few-shot learning scenarios and DART only explored limited $K$ values. Therefore, this project conducted comprehensive experiments on six datasets and various few-shot learning settings. The first empirical evidence indicates that automated prompting methods do not consistently outperform manual prompting, highlighting the importance of using manual prompting as a baseline in this area of research, in addition to fine-tuning. Based on these results, a paper I co-authored has been accepted at the ACL conference\footnote{The Annual Meeting of the Association for Computational Linguistics (ACL): https://2023.aclweb.org/}.

This thesis contributes to the existing literature, the BtoP method \cite{Lei22}, on the vulnerability of prompting models by exploring all three prompting models. New findings suggest that differential prompting is more robust than discrete prompting. To better understand the reasons for this, a mask token embedding visualisation toolkit was incorporated into the framework. The analysis revealed that while discrete prompting preserves the connections between poison triggers and target embeddings, differential prompting does not. 

Furthermore, controlled experiments are conducted to examine the efficacy of different backdoor trigger designs. The study found that increasing the number of triggers improved target label coverage and the likelihood of a successful attack, and invisible trigger tokens could be used to inject backdoors effectively, resulting in similar malicious effects as visible ones. These novel findings highlight the security vulnerabilities in discrete prompting models. I am working with my supervisors to prepare a manuscript featuring these results for the NeurIPS conference\footnote{Conference on Neural Information Processing Systems (NeurIPS): https://nips.cc/}. 

\section{Future Work}
\vspace{-0.5em}
\subsubsection{Effective backdoor attacks on differential prompting}
\vspace{-0.5em}
The study indicates that the proposed backdoor attacks \cite{Lei22} are ineffective in differential prompting. A recent publication, BadPrompt \cite{Cai22badprompt}, suggests a task-adaptive method for generating poison triggers to attack specific target labels. However, this approach violates the threat model in \Cref{sec:prep-threat-model}, which assumes that attackers have no knowledge of the downstream tasks. Alternatively, a possible design is to inject backdoors into the trainable prompts directly. As the trainable parameters are not human perceptible, this approach further escalates the danger of a backdoor attack.
\vspace{-0.5em}
\subsubsection{Poison datasets on the internet}
\vspace{-0.5em}
The proposed backdoor attacks \cite{Lei22} involve poisoning a local copy of a publicly available dataset, such as \textit{WikiText}, and then re-training the PLM to insert the backdoor. Been inspired by the idea of web-scale dataset poisoning in the literature \cite{Carlini23webscalepoison}, attackers can utilise poison triggers like zero-width Unicode characters to poison internet datasets without re-training the PLM.
\vspace{-0.5em}
\subsubsection{Potential defences and countermeasures}
\vspace{-0.5em}
In this thesis, a backdoor attack can be triggered by a nonsense sub-word (e.g., \texttt{cf}) or a zero-width Unicode character. While filtering out these characters from inputs may serve as a user-end defence, it can be bypassed with poison triggers in the form of specific patterns of sensible words or characters, such as repeated sequences like \texttt{"c f c f c f"}. 

\section{Lessons Learnt}
Through this project, I learned the significance of being flexible in exploring beyond the initial proposal. The implementation phase led to two additional project extensions based on experimental results. Additionally, my experience working with deep neural networks emphasised the need for a thorough and methodical testing strategy.

This project also taught me the importance of incorporating slack periods in project planning to manage unexpected issues and reflect on previous results effectively. Upon reflection, one caveat in my implementation is the absence of an automated pipeline to transfer experimental results to a database. I plan to implement a systematic approach to this task for future projects.

As a last word, the interconnection between machine learning and security is a fascinating area with tremendous potential for further exploration. I am eager to continue delving into this domain in the future.